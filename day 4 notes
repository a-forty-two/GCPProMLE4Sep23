Polynomial kernels example (SVM): https://www.youtube.com/watch?v=OdlNM96sHio

For discrete data points-> Categorical Cross-entropy also known as Log Loss. Them minus sign indicates its a LOSS not a profit. 

For continuous data point if you need classification-> SPARSE CATEGORICAL CROSS ENTROPY or Sparse Log Loss 

cross entropy/loss loss: 

  https://dasha.ai/en-us/blog/log-loss-function#:~:text=After%20taking%20the%20logarithm%20of,the%20%22logistic%20error%20function%22
  
  https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a


10 mins to pandas: https://pandas.pydata.org/docs/user_guide/10min.html

Parameters: weights and bias 
2 types of parameters:
  1. Trainable Parameters-> values are going to be adjusted during training
  2. Non Trainable -> during training their values CAN't be adjusted 
              
tf.variable -> weights and bias 

tf.constant -> values that CANNOT BE ADJUSTED-> as they are read-only 
            -> columns from your table (ACTUAL DATA THAT IS BEING LEARNT UPON)


y = m1x1 + m2x2 + m3x3 + b

tf.constant -> x1,x2,x3 (features or dimensions)
tf.variable -> m1,m2,m3,b (Parameters)

to reduce to error per epoch/iteration of learning: we use LOSS and OPTIMIZER functions

LOSS FUNCTION-> calculates how to know the error 
                -> MSE, RMSE, MAE, Cross Entropy


OPTIMIZER FUNCTION-> calculates how to reduce the error
                -> GD, AdaGRAD, ADAM, RMSProp


initialLR -> initial learning rate with which we start to train our model 
finalLR -> if i have found the BEST possible answer, then I don't want any further movement
            -> if my lr = 0, then there will be NO FURTHER movement 
retardation rate = decay rate = negative acceleration 

v = u + at 

final_LR = initial_LR - retardation_rate * epoch 

finalLR = initLR - retardation_rate * epoch 
retardation_rate = ( initLR - finalLR ) / epoch 

we want finalLR to be 0,

retardation_rate = (initLR - 0)/epoch = initialLR / epochs 


























